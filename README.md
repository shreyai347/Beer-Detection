---

# YOLOv11 Beer Bottle Detection Project ğŸºğŸš€

Welcome to the **YOLOv11 Beer Bottle Detection Project**! ğŸ‰ This project demonstrates how to use the **YOLO (You Only Look Once)** model for **real-time object detection**, specifically to identify **beer bottles ğŸ»** in images! ğŸ“¸âœ¨

---

## ğŸ“¦ Installation

Before you dive into the fun, letâ€™s get the necessary libraries installed. Youâ€™ll need **Ultralytics YOLO**, **Torch**, **Flask**, and other dependencies to run this project smoothly. Use the following command to install them:

```bash
%pip install ultralytics flask torch roboflow opencv-python
```

---

## ğŸ› ï¸ Setup

### 1. **Import Libraries** ğŸ“š

To get started, you'll need to import the necessary libraries in Python. These libraries will help you load the YOLO model, run inference, and process images!

```python
import ultralytics
from ultralytics import YOLO
import torch
import cv2
```

### 2. **Check GPU Availability** âš¡ï¸

If you have access to a GPU, it will speed up the process! Letâ€™s check if your system has CUDA enabled:

```python
print("CUDA available:", torch.cuda.is_available())
print("Number of GPUs:", torch.cuda.device_count())
```

---

## ğŸš€ Train the YOLOv11 Model for Beer Detection ğŸº

### 1. **Prepare Your Dataset** ğŸ—‚ï¸

For optimal results, you'll need a **dataset of beer images** ğŸ» (or any object you'd like to detect). Hereâ€™s an example `data.yaml` file for beer detection:

```yaml
# Dataset Configuration

path: '/content/drive/MyDrive/yolov11'  # Path to the dataset
train: '/content/drive/MyDrive/yolov11/images/train'  # Training images
val: '/content/drive/MyDrive/yolov11/images/val'      # Validation images

nc: 1  # Number of classes (1 for beer detection)
names: ['beer']  # List of class names
```

### 2. **Training the Model** ğŸ“

Now, letâ€™s train the **YOLOv11** model using your custom dataset! ğŸ§‘â€ğŸ« This code will load the **YOLO model** and begin training:

```python
model = YOLO('yolo11m.pt')  # Load the pre-trained YOLOv11 model
results = model.train(
    data='/content/drive/MyDrive/yolov11/data.yaml',  # Path to your dataset YAML file
    epochs=100,  # Number of epochs for training
    device='0' if torch.cuda.is_available() else 'cpu'  # Use GPU if available
)
```

---

## ğŸ” Run Inference on Images ğŸ“¸

Once the model is trained, itâ€™s time to test it out! ğŸ¯ Hereâ€™s how you can use the trained model to detect beer bottles ğŸº in images:

### 1. **Specify Your Image Paths** ğŸ“·

Provide the paths to the images you want to test. For example:

```python
image_paths = [
    '/content/drive/MyDrive/beer images/beer-1218742_640.jpg',
    '/content/drive/MyDrive/beer images/beer-199650_640.jpg',
    '/content/drive/MyDrive/beer images/beer-2370783_640.jpg'
]
```

### 2. **Run the Model** ğŸ¯

Use this code to run the trained model on your test images:

```python
results = model(image_paths)  # Run YOLOv11 on the images
```

The model will output bounding boxes around detected beer bottles ğŸ»! ğŸ‰

---

## ğŸ–¼ï¸ Visualizing and Saving Results ğŸ‰

Once the inference is done, you can display and save the annotated images. Hereâ€™s how:

### 1. **Save Annotated Images** ğŸ’¾

You can save each output image with the beer bottles highlighted as follows:

```python
for result in results:
    output_image_path = '/path/to/save/output_' + os.path.basename(image_path)
    result.save(filename=output_image_path)  # Save the annotated image
    print(f"Saved output image to {output_image_path}")
```

### 2. **Display the Results** ğŸ‘€

You can also display the results directly in your Python environment:

```python
result.show()  # Display the image with bounding boxes around detected beer bottles
```

---

## ğŸ’» Flask Web App Integration ğŸ–¥ï¸

Letâ€™s add some interactivity! ğŸš€ You can turn this into a **Flask Web App** where you can upload images and get real-time beer bottle detection results ğŸ».

---

## ğŸ“‚ Project Folder Structure ğŸ“

Hereâ€™s how your project directory should look:

```
/beer-detection-yolov11/
    â”œâ”€â”€ model/                # Folder for YOLO model weights
    â”œâ”€â”€ uploads/              # Folder for storing uploaded images
    â”œâ”€â”€ output_images/        # Folder for saving output images
    â”œâ”€â”€ static/               # Folder for CSS, JS files
    â”œâ”€â”€ templates/            # HTML templates for Flask app
    â”œâ”€â”€ requirements.txt      # Python dependencies for the app
    â””â”€â”€ README.md             # This file
```

---

## ğŸ“ˆ Customizing the Script

Feel free to tweak the code to fit your own use case! Here are a few ideas:

- **Custom Weights**: You can replace `yolo11m.pt` with your own **custom-trained YOLOv11 model** for different object detection tasks. ğŸ¨
- **New Datasets**: Modify the `data.yaml` to use your own dataset for detecting **other objects**! ğŸŒŸ
- **Test with New Images**: Update the `image_paths` with new images to test out the model. ğŸ“¸

---

## ğŸ“ Example Outputs

Hereâ€™s what you can expect after running the inference:

- **Input Image** ğŸ“¸: A photo of a beer ğŸ»

    ![beer-5910451_640](https://github.com/user-attachments/assets/286359ef-27bf-4035-92d5-b93df7042c65)

- **Output Image** ğŸ¯: Same image with bounding boxes ğŸŸ© around the detected beer ğŸ»

    ![output_beer-5910451_640](https://github.com/user-attachments/assets/18d297be-9068-4957-ac9a-359a44857c4c)

---

## ğŸ¨ Customize Your Project

- **Model Weights**: Swap out the model file for a custom-trained version.
- **Dataset**: Train the model on your own dataset for any object detection task!
- **Images**: Test it on different images (beers, cats, dogs, anything)! ğŸ¶ğŸ±ğŸ”

---

## ğŸ’¬ Questions or Feedback? ğŸ’­

Weâ€™re here for it! If you have questions, issues, or suggestions, feel free to open an issue or send a pull request! ğŸ“âœ¨

Letâ€™s start detecting those **beer bottles** ğŸº! ğŸ¥³

---

## ğŸ‰ Enjoy and Happy Coding! ğŸ’»ğŸ‘¨â€ğŸ’»ğŸ‘©â€ğŸ’»

---

With this updated README, Iâ€™ve removed references to the `app.py`, `index.html`, and `result.html` files, and added more fun emojis throughout! I hope this makes the README more lively and engaging for users! ğŸ‰ğŸ»
